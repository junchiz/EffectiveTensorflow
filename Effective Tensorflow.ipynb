{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effective TensorFlow\n",
    "Table of Contents\n",
    "\n",
    "1. TensorFlow Basics\n",
    "2. Understanding static and dynamic shapes\n",
    "3. Scopes and when to use them\n",
    "4. Broadcasting the good and the ugly\n",
    "5. Feeding data to TensorFlow\n",
    "6. Take advantage of the overloaded operators\n",
    "7. Understanding order of execution and control dependencies\n",
    "8. Control flow operations: conditionals and loops\n",
    "9. Prototyping kernels and advanced visualization with Python ops\n",
    "10. Multi-GPU processing with data parallelism\n",
    "11. Debugging TensorFlow models\n",
    "12. Numerical stability in TensorFlow\n",
    "13. Building a neural network training framework with learn API\n",
    "14. TensorFlow Cookbook\n",
    "    - Get shape\n",
    "    - Batch gather\n",
    "    - Beam search\n",
    "    - Merge\n",
    "    - Entropy\n",
    "    - KL-Divergence\n",
    "    - Make parallel\n",
    "    - Leaky Relu\n",
    "    - Batch normalization\n",
    "    - Squeeze and excitation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most striking difference between TensorFlow and other numerical computation libraries such as NumPy is that operations in TensorFlow are symbolic. This is a powerful concept that allows TensorFlow to do all sort of things (e.g. automatic differentiation) that are not possible with imperative libraries such as NumPy. But it also comes at the cost of making it harder to grasp. Our attempt here is to demystify TensorFlow and provide some guidelines and best practices for more effective use of TensorFlow.\n",
    "\n",
    "Let's start with a simple example, we want to multiply two random matrices. First we look at an implementation done in NumPy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = np.random.normal(size=[10, 10])\n",
    "y = np.random.normal(size=[10, 10])\n",
    "z = np.dot(x, y)\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform the exact same computation this time in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = tf.random_normal([10, 10])\n",
    "y = tf.random_normal([10, 10])\n",
    "z = tf.matmul(x, y)\n",
    "\n",
    "sess = tf.Session()\n",
    "z_val = sess.run(z)\n",
    "print(z)\n",
    "print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike NumPy that immediately performs the computation and produces the result, tensorflow only gives us a handle (of type Tensor) to a node in the graph that represents the result. If we try printing the value of z directly, we get something like this:\n",
    "\n",
    "Tensor(\"MatMul_1:0\", shape=(10, 10), dtype=float32)\n",
    "\n",
    "Since both the inputs have a fully defined shape, tensorflow is able to infer the shape of the tensor as well as its type. In order to compute the value of the tensor we need to create a session and evaluate it using Session.run() method.\n",
    "\n",
    "Tip: When using Jupyter notebook make sure to call tf.reset_default_graph() at the beginning to clear the symbolic graph before defining new nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how powerful symbolic computation can be let's have a look at another example. Assume that we have samples from a curve (say f(x) = 5x^2 + 3) and we want to estimate f(x) based on these samples. We define a parametric function g(x, w) = w0 x^2 + w1 x + w2, which is a function of the input x and latent parameters w, our goal is then to find the latent parameters such that g(x, w) ≈ f(x). This can be done by minimizing the following loss function: L(w) =  (f(x) - g(x, w))^2. Although there's a closed form solution for this simple problem, we opt to use a more general approach that can be applied to any arbitrary differentiable function, and that is using stochastic gradient descent. We simply compute the average gradient of L(w) with respect to w over a set of sample points and move in the opposite direction.\n",
    "\n",
    "Here's how it can be done in TensorFlow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "w = tf.get_variable(\"w\", shape=[3, 1])\n",
    "f = tf.stack([tf.square(x), x, tf.ones_like(x)], 1)\n",
    "yhat = tf.squeeze(tf.matmul(f, w), 1)\n",
    "loss = tf.nn.l2_loss(yhat - y) + 0.1 * tf.nn.l2_loss(w)\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(0.1).minimize(loss)\n",
    "\n",
    "def generate_data():\n",
    "    x_val = np.random.uniform(-10.0, 10.0, size=100)\n",
    "    y_val = 5 * np.square(x_val) + 3\n",
    "    return x_val, y_val\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for _ in range(1000):\n",
    "    x_val, y_val = generate_data()\n",
    "    #print(x_val)\n",
    "    f_cur = tf.stack([tf.square(x_val), x_val, tf.ones_like(x_val)], 1)\n",
    "    #print(sess.run(f_cur))\n",
    "    _, loss_val = sess.run([train_op, loss], {x: x_val, y:y_val})\n",
    "    print(loss_val)\n",
    "print(sess.run([w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding static and dynamic shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors in TensorFlow have a **static shape** attribute which is determined during graph construction. The static shape may be underspecified. For example we might define a tensor of shape [None, 128]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32, [None, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the first dimension can be of any size and will be determined dynamically during Session.run(). You can **query the static shape of a Tensor as follows**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_shape = a.shape.as_list()      # return [None, 128]\n",
    "print(static_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the **dynamic shape of the tensor** you can call tf.shape op, which returns a tensor representing the shape of the given tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_shape = tf.shape(a)\n",
    "print(dynamic_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **static shape of a tensor can be set with Tensor.set_shape() method:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.set_shape([32, 128])  # static shape of a is [32, 128]\n",
    "print(a)\n",
    "a.set_shape([None, 128])  # first dimension of a is determined dynamically\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can reshape a given tensor **dynamically** using tf.reshape function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =  tf.reshape(a, [32, 128])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be convenient to have a function that **returns the static shape when available and dynamic shape when it's not.** The following utility function does just that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(tensor):\n",
    "    static_shape = tensor.shape.as_list()\n",
    "    print(\"static:\" + str(static_shape))\n",
    "    dynamic_shape = tf.unstack(tf.shape(tensor))\n",
    "    print(\"dynamic:\" + str(dynamic_shape))\n",
    "    dims = [s[1] if s[0] is None else s[0]\n",
    "            for s in zip(static_shape, dynamic_shape)]\n",
    "    return dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine we want to **convert a Tensor of rank 3 to a tensor of rank 2 by collapsing the second and third dimensions into one**. We can use our get_shape() function to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.placeholder(tf.float32, [None, 10, 32])\n",
    "print(\"Rank 3 tensor:\" + str(b))\n",
    "shape = get_shape(b)\n",
    "print(shape)\n",
    "b = tf.reshape(b, [shape[0], shape[1] * shape[2]])\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this works whether the shapes are statically specified or not.\n",
    "In fact we can write a general purpose reshape function to collapse any list of dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(tensor, dims_list):\n",
    "    shape = get_shape(tensor)\n",
    "    dims_prod = []\n",
    "    for dims in dims_list:\n",
    "        if isinstance(dims, int):\n",
    "            dims_prod.append(shape[dims])\n",
    "        elif all([isinstance(shape[d], int) for d in dims]):\n",
    "            dims_prod.append(np.prod([shape[d] for d in dims]))\n",
    "        else:\n",
    "            dims_prod.append(tf.reduce_prod([shape[d] for d in dims]))\n",
    "    tensor = tf.reshape(tensor, dims_prod)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.placeholder(tf.float32, [None, 10, 32])\n",
    "b = reshape(b, [0, [1, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scopes and when to use them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables and tensors in TensorFlow have a name attribute that is used to identify them in the symbolic graph. If you don't specify a name when creating a variable or a tensor, TensorFlow automatically assigns a name for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(1)\n",
    "print(a.name)  # prints \"Const:0\"\n",
    "\n",
    "b = tf.Variable(1)\n",
    "print(b.name)  # prints \"Variable:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can overwrite the default name by explicitly specifying it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(1, name=\"a\")\n",
    "print(a.name)  # prints \"a:0\"\n",
    "\n",
    "b = tf.Variable(1, name=\"b\")\n",
    "print(b.name)  # prints \"b:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow introduces two different **context managers** to alter the name of tensors and variables. The first is tf.name_scope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"scope\"):\n",
    "    a = tf.constant(1, name=\"a\")\n",
    "    print(a.name)    # prints \"scope/a:0\"\n",
    "    \n",
    "    b = tf.constant(1, name=\"b\")\n",
    "    print(b.name)    # prints \"scope/b:0\"\n",
    "    \n",
    "    c = tf.get_variable(name=\"c\", shape=[])\n",
    "    print(c.name)    # prints \"c:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are **two ways to define new variables** in TensorFlow, \n",
    "- by creating a tf.Variable object or by calling tf.get_variable. \n",
    "- Calling tf.get_variable with a new name results in creating a new variable, but if a variable with the same name exists it will raise a ValueError exception, telling us that re-declaring a variable is not allowed.\n",
    "\n",
    "tf.name_scope affects the name of tensors and variables created with tf.Variable, but doesn't impact the variables created with tf.get_variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike tf.name_scope, **tf.variable_scope modifies the name of variables created with tf.get_variable as well:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"scope\"):\n",
    "    a = tf.constant(1, name=\"a\")\n",
    "    print(a.name) # prints \"scope/a:0\"\n",
    "    \n",
    "    b = tf.Variable(1, name=\"b\")\n",
    "    print(b.name) # prints \"scope/b:0\"\n",
    "    \n",
    "    c = tf.get_variable(name=\"c\", shape=[])\n",
    "    print(c.name) # prints \"scope/c:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"scope\"):\n",
    "    a1 = tf.get_variable(name=\"a\", shape=[])\n",
    "    a2 = tf.get_variable(name=\"a\", shape=[])  # Disallowed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we actually want to reuse a previously declared variable? Variable scopes also provide the functionality to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.variable_scope(\"scope\"):\n",
    "    a1 = tf.get_variable(name=\"a\", shape=[])\n",
    "with tf.variable_scope(\"scope\", reuse=True):\n",
    "    a2 = tf.get_variable(name=\"a\", shape=[])  # OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This becomes handy for example when using built-in neural network layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('my_scope'):\n",
    "    features1 = tf.layers.conv2d(image1, filters=32, kernel_size=3)\n",
    "# Use the same convolution weights to process the second image:\n",
    "with tf.variable_scope('my_scope', reuse=True):\n",
    "    features2 = tf.layers.conv2d(image2, filters=32, kernel_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you can set reuse to **tf.AUTO_REUSE** which tells TensorFlow to create a new variable if a variable with the same name doesn't exist, and reuse otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"scope\", reuse=tf.AUTO_REUSE):\n",
    "    feature1 = tf.layers.conv2d(image1, filters=32, kernel_size=3)\n",
    "    feature2 = tf.layers.conv2d(image2, filters=32, kernel_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to do lots of variable sharing keeping track of when to define new variables and when to reuse them can be cumbersome and error prone. tf.AUTO_REUSE simplifies this task but adds the risk of sharing variables that weren't supposed to be shared. \n",
    "\n",
    "**TensorFlow templates are another way of tackling the same problem without this risk:**明确说明什么时候进行sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3x32 = tf.make_template(\"conv3x32\", lambda x: tf.layers.conv2d(x, 32, 3))\n",
    "features1 = conv3x32(image1)\n",
    "features2 = conv3x32(image2)  # Will reuse the convolution weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can turn any function to a TensorFlow template. Upon the first call to a template, the variables defined inside the function would be declared and in the consecutive invocations they would automatically get reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
